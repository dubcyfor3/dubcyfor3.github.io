---
permalink: /
title: "Research Experience"
author_profile: true
redirect_from: 
  - /research_experience/
  - /research_experience.html
---

# Graduate Research Assistant at Duke University

---

### Accelerating Spiking Neural Networks via Redundancy Removal Method
*Sept. 2023 – Aug. 2024*

- Developed a novel product sparsity paradigm that significantly reduced computational complexity in SNNs.
- Architected an efficient pipelined hardware design optimized for processing product sparsity paradigm.
- Accelerated a wide range of SNNs and achieved **7.4× speedup** over state-of-the-art (SOTA) SNN accelerator.

---

### Accelerator for General Matrix Multiplication in Quantized DNN
*Aug. 2024 – Present*

- Developed innovative methods to identify and exploit result reuse opportunities in single-bit matrix multiplication operations.
- Architected a novel hardware design that maximizes computational efficiency through optimized execution ordering and parallel processing.
- Achieves **9.81× speedup** and **2.26× energy reduction** compared to SOTA quantization accelerators.

---

### Efficient Cache Compression for Large Language Models on GPU
*Feb. 2024 – Nov. 2024*

- Pioneered an entropy-based K-means quantization technique for efficient compression of LLM weights and KV cache.
- Designed and implemented a parallel Huffman coding architecture enabling efficient L2 cache compression with minimal latency overhead.
- Achieves up to **2.9× speedup** over the SOTA quantization framework while maintaining SOTA LLM accuracy.

---

### Efficient Spiking Neural Networks via Hierarchical Sparsity Pattern
*Aug. 2024 – Present*

- Developed a novel pattern-based hierarchical sparsity framework that effectively leverages inherent activation structures in SNNs.
- Designed architecture incorporating parallel processing capabilities for efficient hierarchical sparsity generation and handling.
- Achieved a **3.45× speedup** and a **4.93× improvement in energy efficiency** compared to SOTA SNN accelerators.

---

# Research Assistant with Prof. Yuan Xie, University of California, Santa Barbara

---

### A Systolic-Array-Friendly Design for Sparse LU Factorization
*Jun. 2022 – Jan. 2023*

- Introduced path decomposition techniques to enhance parallel processing capabilities in sparse LU decomposition.
- Developed a hybrid data storage format enabling efficient conversion of sparse matrix operations to dense computations.
- Increased utilization rate of systolic array in sparse LU factorization by **2.25×**.

---

# Research Assistant with Prof. Yu Wang, Tsinghua University

---

### Triangle Counting Acceleration with Software–Hardware Co-Design
*Feb. 2022 – Sept. 2022*

- Proposed an advanced preprocessing methodology to minimize off-chip memory access overhead.
- Developed a specialized CAM-based hardware architecture to accelerate set intersection operations in triangle counting.
- Achieved **39×, 27×, and 78× speedup** against SOTA CPU, GPU, and PIM baseline.

### Graph Mining Acceleration by Near-Memory-Computing Architecture
*Sept. 2021 – Feb. 2022*

- Optimized graph mining algorithms to minimize computational complexity while maximizing parallel processing capabilities.
- Architected a high-throughput systolic architecture specifically designed for efficient set intersection operations.
- Proposed a Near-Memory-Computing architecture to address data transfer bottlenecks in graph processing and achieved **3.61× speedup** over the SOTA graph mining architecture.

### Graph Neural Network Algorithm for Subgraph Counting
*Jan. 2023 – Jun. 2023*

- Designed and implemented an innovative graph partitioning strategy to enhance pattern counting accuracy.
- Developed specialized heterogeneous and learnable gates within GNN architecture to improve subgraph counting precision.
- Outperformed SOTA neural methods with **137× improvement in the mean squared error**, while maintaining the polynomial complexity.

